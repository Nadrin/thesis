\chapter{Monte Carlo integration}
Monte Carlo integration is a method of approximating the value of an integral which is difficult or impossible to solve analytically, thus it is usually the solution of choice for approximating the \emph{light transport equation} \footnote{While there are other valid approaches to solving the LTE such as finite element methods (e.g. radiosity), those are not in the scope of this work.}. Due to the high dimensionality of the LTE integral traditional numerical integration methods like quadrature rules are inefficient and converge very slowly to the approximate solution.

\section{The Monte Carlo estimator}
Let $f: I \rightarrow \mathbb{R}$ be a real-valued function defined over the interval $I$ and $a,b \in I$. Monte Carlo integration is a mean to estimate the integral of the form:
\begin{equation}
  F = \int_{a}^{b} f(x)dx.
\end{equation}

The basis for deriving the Monte Carlo estimator is the \emph{mean value theorem} for integration:
\begin{thm}
If $f(x)$ is continuous over $[a, b]$, then there exists a number $c \in (a, b)$ such that:
\begin{equation}
\label{eq:meanvalue}
  \frac{1}{b-a} \int_{a}^{b} f(x)dx = f(c).
\end{equation}
\end{thm}
Rearranging the terms of equation \ref{eq:meanvalue} yields:
\begin{equation}
  \int_{a}^{b} f(x)dx = (b-a) \cdot f(c) = F,
\end{equation}
which has simple interpretation that the area under the curve is the width of its base $(b-a)$ times the average ``height'' $f(c)$. To estimate the value of $f(c)$ one could evalute the function $f(x)$ at random points in the interval $[a,b]$ and compute the average of the sum of samples.

\begin{df}[Monte Carlo estimator]
  Let $X_{i} \in [a,b]$ be a random variable drawn from some distribution with probability density function $p(X_{i})$ and let $f: I \rightarrow \mathbb{R}$ be a function defined over the interval $I$, and $a,b \in I$. The Monte Carlo estimator has the form \parencite{veach97}:
\begin{equation}
\label{eq:mcestimator}
  F_{N} = \frac{1}{N} \sum_{i=1}^{N} \frac{f(X_{i})}{p(X_{i})}.
\end{equation}
\end{df}
With that definition the expected value of the estimator $F_{N}$ is the value of the integral $F$:
\begin{eqnarray}
  E[F_{N}] &=& E\left[ \frac{1}{N} \sum_{i=1}^{N} \frac{f(X_{i})}{p(X_{i})} \right]
  = \frac{1}{N} \sum_{i=1}^{N} E \left[ \frac{f(X_{i})}{p(X_{i})} \right] = \nonumber \\
  &=& \frac{1}{N} \sum_{i=1}^{N} \int_{a}^{b} \frac{f(x)}{p(x)} p(x) dx
  = \frac{1}{N} \sum_{i=1}^{N} \int_{a}^{b} f(x) dx = \nonumber \\
  &=& \int_{a}^{b} f(x)dx = F.
\end{eqnarray}
For functions with higher number of dimensions random samples $X_{i}$ are drawn from \emph{joint probability density function}.

Average case rate of convergence for Monte Carlo estimator is $O(\sqrt{N})$, where $N$ is the number of samples taken. Formal variance and error analysis of Monte Carlo integration is out of scope of this work. Treatment of this topic can be found in \cite{robert2004}.

\section{Sampling from arbitrary distributions}
Vast majority of pseudo-random number generation algorithms output numbers drawn from \emph{standard uniform distribution}. Random variable with such distribution is commonly written as $\xi \in [0,1)$ and it takes on all values in its domain with equal probability.

Ability to draw random samples from chosen probability distribution is crucial in order to evaluate the Monte Carlo estimator in equation \ref{eq:mcestimator}. This section will present basics of drawing samples from arbitrary distributions given only samples drawn from the standard uniform distribution.

\subsection{The inversion method}
The inversion method or the \emph{inverse transform sampling} \parencite{devroye86} is a method of generating samples according to some distribution by transforming the canonical uniform random variable $\xi$.
\begin{thm}[The inversion principle]
  Let $P(x)$ be a continuous distribution function on $\mathbb{R}$ with inverse $P^{-1}$ defined by
  \begin{equation}
    P^{-1}(u) = \inf \left\{ x:P(x)=u, 0 < u < 1 \right\}.
  \end{equation}
If $\xi \in [0,1]$ is a uniform random variable, then $P^{-1}(\xi)$ has distribution function $P$.
\end{thm}

Let $\xi_{i}$ be a uniformly distributed random number obtained via some pseudo-random number generator (RNG). Given the above theorem it is possible to draw a sample $X_{i}$ from an arbitrary probability density function $p(x)$ by transforming $\xi_{i}$ in the following way:
\begin{enumerate}
\item Compute the cumulative distribution function $P(x) = \int_{-\infty}^{x} p(x') \,dx'$.
\item Find the inverse $P^{-1}(x)$.
\item Compute $X_{i} = P^{-1}(\xi_{i})$.
\end{enumerate}

\subsection{Transforming between distributions}
When sampling from multidimensional distributions it is often useful to transform a random variable with some function $f$. For example one might wish to express solid angles in terms of points in spherical coordinates. When dealing with such transformations it is curcial to know the relationship between corresponding density functions.

Consider a pair of n-dimensional random variables $X$, $Y$ with corresponding probability density functions $p_{x}(x)$ and $p_{y}(y)$. Let $Y = T(X)$ where $T(x)=\langle T_{1}(x), \dots, T_{n}(x) \rangle$ is a bijection. The densities are then related by \parencite{phar2010}:
\begin{equation}
\label{eq:pdf_transform}
  p_{y}(y)=p_{y}(T(x)) = \frac{p_{x}(x)}{|J_{T}(x)|},
\end{equation}
where $|J_{T}(x)|$ is the absolute value of the determinant of $T$'s Jacobian matrix.

\section{2D sampling of random variables}
A number of two-dimensional distributions are commonly used when approximating solution to the LTE using Monte Carlo integration. This section introduces methods of uniformly sampling those distributions.

\subsection{Unit hemisphere}
Consider choosing a direction vector on a hemisphere uniformly with respect to solid angle. Uniform distribution implies that the density function is constant and it also, by definition, integrates to one.
\begin{eqnarray}
  \int_{\Omega} p(\omega) \,d\omega &=& 1 \nonumber \\
  c \int_{\Omega} \,d\omega &=& 1 \nonumber \\
  p(\omega) = c &=& \frac{1}{2\pi}
\end{eqnarray}
The next step is to express the density function with respect to spherical coordinates using equation \ref{eq:dw2spherical}:
\begin{eqnarray}
  p(\theta, \phi) \, d\theta d\phi &=& p(\omega) \,d\omega \nonumber \\
  p(\theta, \phi) &=& \sin\theta \,p(\omega),
\end{eqnarray}
subsituting for $p(\omega)$ yields
\begin{equation}
  p(\theta, \phi) = \sin\theta \,\frac{1}{2\pi}.
\end{equation}
It is easly seen that the two-dimensional density function $p(\theta, \phi)$ is seperable and thus can be expressed as a product of two one-dimensional densities: $p(\theta) = \sin\theta$ and $p(\phi) = \frac{1}{2\pi}$ which can be sampled independently using the inversion method.

Computing the respective cumulative distribution functions:
\begin{eqnarray}
  P(\theta) &=& \int_{-\infty}^{\theta} \sin\theta' \,d\theta' = 1 - \cos\theta \\
  P(\phi) &=& \int_{-\infty}^{\phi} \frac{1}{2\pi} \,d\phi' = \frac{\phi}{2\pi},
\end{eqnarray}
and their inversions yields:
\begin{eqnarray}
  \theta &=& P^{-1}(\theta) = \cos^{-1}(1 - \xi_{1}) \label{eq:2dsample_theta} \\
  \phi &=& P^{-1}(\phi) = 2\pi \xi_{2},
\end{eqnarray}
where $\xi_{1}$ and $\xi_{2}$ are random numbers drawn from standard uniform distribution. Note that if $\xi_{i}$ is uniformly distributed random number over $[0,1]$ then $1-\xi_{i}$ is also uniformly distributed. This can be used to further simplify equation \ref{eq:2dsample_theta}:
\begin{equation}
  \theta = \cos^{-1}(1-\xi_{1}) = \cos^{-1}(\xi_{1}).
\end{equation}
Converting $\theta$ and $\phi$ into Cartesian coordinates yields final formulae for uniformly sampling a hemisphere:
\begin{eqnarray}
  x &=& \sin\theta \cos\phi = \cos(2\pi \xi_{2}) \sqrt{1 - \xi_{1}^{2}} \nonumber \\
  y &=& \sin\theta \sin\phi = \sin(2\pi \xi_{2}) \sqrt{1 - \xi_{1}^{2}} \\
  z &=& \cos\theta = \xi_{1}. \nonumber
\end{eqnarray}

\subsection{Unit disk}
Now consider choosing a point on a disk uniformly with respect to area. A point on a disk can be expressed using \emph{polar coordinates}:
\begin{eqnarray}
\label{eq:polar}
  x &=& r \cos\theta \nonumber \\
  y &=& r \sin\theta
\end{eqnarray}
The Jacobian determinant of this transformation is
\begin{equation}
  J_{T} = \det \frac{\partial(x,y)}{\partial(r,\theta)} =
  \left| 
    \begin{array}{cc}
      \cos\theta & -r \sin\theta \\
      \sin\theta & r \cos\theta
    \end{array}
  \right| = r.
\end{equation}
Plugging $J_{T}$ into equation \ref{eq:pdf_transform} and rearranging the terms yields
\begin{equation}
\label{eq:pdf_disk}
  p(r, \theta) = r\,p(x, y),
\end{equation}
where $p$ is probability density function. Uniform distribution implies that $p(x,y)$ needs to be constant.

Differential area on a disk can be expressed in terms of $J_{T}$:
\begin{equation}
  dA = dx\,dy = J_{T}\,dr\,d\theta = r\,dr\,d\theta.
\end{equation}
Analogous to the hemisphere case the density function integrates to one, this time, over the domain of a unit disk:
\begin{eqnarray}
  \int_{0}^{2\pi} \int_{0}^{1} p(x,y) \,r \,dr d\theta &=& 1 \nonumber \\
  p(x,y) \int_{0}^{2\pi} \int_{0}^{1} r \,dr d\theta &=& 1 \nonumber \\
  p(x,y) = c &=& \frac{1}{\pi},
\end{eqnarray}
therefore from equation \ref{eq:pdf_disk}:
\begin{equation}
  p(r, \theta) = \frac{r}{\pi}.
\end{equation}

Instead of deriving unit disk sampling formulae using the inversion method as before I will use the technique called \emph{concentric disk mapping} \parencite{shirley97}. Concentric map as opposed to traditional polar map is a \emph{low distortion} transformation preserving shapes and fractional areas.

Consider a pair $\xi_{1}$, $\xi_{2}$ of random numbers drawn from standard uniform distribution and function $f$ mapping $\langle \xi_{1}, \xi_{2} \rangle$ onto $[-1,1]^{2}$ square:
\begin{equation}
  f(\xi_{1}, \xi_{2}) = \langle 2\xi_{1} - 1, 2\xi_{2} - 1 \rangle = \langle a, b \rangle.
\end{equation}
The $[-1,1]^{2}$ square is divided into four regions by the lines $a=b$ and $a=-b$. Every such region produces an angle of $\frac{\pi}{2}$. The correct mapping formulae are chosen based on the region in which the point $\langle a, b \rangle$ lies. For the first region the mapping is:
\begin{eqnarray}
  r &=& a \nonumber \\
  \theta &=& \frac{\pi}{4} \frac{b}{a}.
\end{eqnarray}
The other mappings are analogous and are omitted for brevity. Refer to \cite{shirley97} for full implementation of all four cases. Note that the sampled point $\langle r, \theta \rangle$ is in polar coordinates and needs to be converted back into Cartesian coordinates with equation \ref{eq:polar}.

\subsection{Cosine weighted unit hemisphere}
The rate of convergence of Monte Carlo integration is highly dependent of how close the sampling distribution ``fits'' the integrated function. For best results it is better to sample from distribution that is similar in shape to the integrand.

Recall that the scattering part of the LTE is weighted by the $\cos\theta$ term therefore a probability density function such that $p(\omega) \propto \cos\theta$ is desirable.

A method to achievie a cosine weighted hemispherical distribution is to uniformly sample a unit disk and then generate direction vectors by projecting the sampled points along the $z$ axis onto unit hemisphere above the disk.
Let $\langle u, v \rangle$ be a uniformly chosen point on a unit disk. The sampling formulae is then \parencite{shirley97}:
\begin{eqnarray}
  x &=& u \nonumber \\
  y &=& v \nonumber \\
  z &=& \sqrt{1 - (u^{2} + v^{2})}.
\end{eqnarray}

\section{Multiple importance sampling}
To improve efficiency of the Monte Carlo estimator even further it is desirable to sample from multiple distributions. This stems from the fact that it is often unknown whether chosen distribution ``fits well'' the integrated function.

One solution to this problem is \emph{multiple importance sampling} which works by evaluating the Monte Carlo estimator several times for multiple distributions and taking weighted average of the results. The usual strategy for evaluating the LTE is to sample both BRDFs and light sources.

Consider evaluating the integral of $f(x)g(x)$ given two sampling distributions $p_{f}$ and $p_{g}$. The new Monte Carlo estimator is then \parencite{phar2010}:
\begin{equation}
  F_{MIS} = 
  \frac{1}{n_{f}} \sum_{i=1}^{n_{f}} \frac{f(X_{i})g(X_{i})w_{f}(X_{i})}{p_{f}(X_{i})} + 
  \frac{1}{n_{g}} \sum_{j=1}^{n_{g}} \frac{f(Y_{j})g(Y_{j})w_{g}(Y_{j})}{p_{g}(Y_{j})},
\end{equation}
where $n_{f}$ is the number of samples taken from the $p_{f}$ distribution, $n_{g}$ is the number of samples taken from the $p_{g}$ distribution and $w_{f}$ and $w_{g}$ are weighting functions.

The weighting functions must be chosen in a way that guarantees that the expected value of $F_{MIS}$ estimator is equal to the integral:
\begin{equation}
  E[F_{MIS}] = \int f(x)g(x) \,dx.
\end{equation}
A generally recommended weighting function is \emph{power heuristic} \parencite{veach97}:
\begin{equation}
  w_{k}(x) = \frac{(n_{k}\,p_{k}(x))^{\beta}}{\sum_{i}(n_{i}\,p_{i}(x))^{\beta}}.
\end{equation}
An optimal choice for the exponent is $\beta = 2$, as noted by Veach. With this value power heuristic scales the contribution of a sample proportional to the value of its probability density function.